{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7594f972-ad33-40de-8ac4-367a5f2e3820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9bdc63-8940-4869-8a65-4bc4c849a9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-05 08:57:46--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240505T125746Z&X-Amz-Expires=300&X-Amz-Signature=3575be07e961f0fe7ab4c208b8263be510088834b7757d07c2b7a76007e46b9c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-05-05 08:57:46--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240505T125746Z&X-Amz-Expires=300&X-Amz-Signature=3575be07e961f0fe7ab4c208b8263be510088834b7757d07c2b7a76007e46b9c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz.4’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  36.4MB/s    in 0.5s    \n",
      "\n",
      "2024-05-05 08:57:47 (36.4 MB/s) - ‘fhv_tripdata_2019-10.csv.gz.4’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downlod the CSV assigned for the homework\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa3fef3-170a-4603-bf89-d9611655615c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 08:57:50 WARN Utils: Your hostname, thinkpad resolves to a loopback address: 127.0.1.1; using 192.168.1.162 instead (on interface wlp3s0)\n",
      "24/05/05 08:57:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/05 08:57:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# launch a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450527f-17d3-43ad-8be1-85cf45037162",
   "metadata": {},
   "source": [
    "### Question 1: what version of Spark are you using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de748ed-ed92-462b-8219-291a262d9403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08971510-7a76-4906-9224-113936e199b6",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons. Repartition the Dataframe to 6 partitions and save it to parquet. What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4cc6c0-b2dd-4129-9830-9614b4a36f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read a preview of the dataset to create a schema\n",
    "preview = pd.read_csv('fhv_tripdata_2019-10.csv.gz', compression='gzip', nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5425e2-6369-43f5-9312-538ca47fc004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('dispatching_base_num', StringType(), True)\n",
      "StructField('pickup_datetime', StringType(), True)\n",
      "StructField('dropOff_datetime', StringType(), True)\n",
      "StructField('PUlocationID', DoubleType(), True)\n",
      "StructField('DOlocationID', DoubleType(), True)\n",
      "StructField('SR_Flag', DoubleType(), True)\n",
      "StructField('Affiliated_base_number', StringType(), True)\n"
     ]
    }
   ],
   "source": [
    "# create a spark df from pandas df and copy the schema for Spark CSV read\n",
    "for col in spark.createDataFrame(preview).schema:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84eda00-2f52-4139-bb94-93f5e1eb23fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# schema with python types\n",
    "schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "types.StructField('PUlocationID', types.DoubleType(), True), \n",
    "types.StructField('DOlocationID', types.DoubleType(), True), \n",
    "types.StructField('SR_Flag', types.DoubleType(), True), \n",
    "types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf6afd7-e875-4304-b2f9-84c42e0c1c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the CSV with spark using above schema\n",
    "df = spark.read.csv('fhv_tripdata_2019-10.csv.gz', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c17b048-41d1-4ad1-a28b-d8535a1ef365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write the partitioned dataset to parquet files \n",
    "os.mkdir('sparktest')\n",
    "df.repartition(6).write.parquet('sparktest/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0bc678-3e94-47d6-85ae-05ce2439b964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00000-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00001-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00002-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00003-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00004-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student 6.4M May  5 08:58 part-00005-e824925b-5f36-4c49-a996-ea9550294121-c000.snappy.parquet\n",
      "-rw-r--r-- 1 student student    0 May  5 08:58 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# check file size\n",
    "!ls -lh sparktest/test.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95598d3-20f3-4cce-a008-71cceb528fde",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "**Count records:** How many taxi trips were there on the 15th of October? Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a3b47b-6c82-47b1-bce1-9cf1844f4066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# method 1 - use SQL\n",
    "\n",
    "# establish view\n",
    "df.createOrReplaceTempView('FHV_trips_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3707b96e-94d5-4917-a3b9-6c24f4f50ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_trips|\n",
      "+-----------+\n",
      "|      62610|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run query on view\n",
    "spark.sql('''\n",
    "SELECT COUNT(*) AS total_trips\n",
    "FROM FHV_trips_2019_10\n",
    "WHERE CAST(pickup_datetime AS DATE) = '2019-10-15'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8588a20-1934-4e0d-a241-cfff3932ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2: python one liner\n",
    "df.filter(df.pickup_datetime.cast('date') == '2019-10-15').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ca02a-49df-4ab8-bba5-76a062eae8b7",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "**Longest trip for each day:** What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfea3b88-4e11-4ab4-93cb-dbac5e7ba454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|longest_trip_hours|\n",
      "+------------------+\n",
      "|            631152|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate the total hours for each trip and show top 1 record\n",
    "spark.sql('''\n",
    "SELECT\n",
    "TIMESTAMPDIFF(HOUR, pickup_datetime, dropoff_datetime) AS longest_trip_hours\n",
    "FROM FHV_trips_2019_10\n",
    "ORDER BY longest_trip_hours DESC\n",
    "''').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d96a5-89b0-4ca7-90a9-51f5722fe275",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "\n",
    "**User Interface:**\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port? **4040**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41ba87-c904-421d-94fb-49cc048d1d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "**Least frequent pickup location zone:**\n",
    "Load the zone lookup data into a temp view in Spark. Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "- East Chelsea\n",
    "- **Jamaica Bay**\n",
    "- Union Sq\n",
    "- Crown Heights North\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97719f8-81d5-4a9e-8302-5cb3ae3e0c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-05 08:58:30--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240505T125830Z&X-Amz-Expires=300&X-Amz-Signature=8093acb066386650e6a963d04427bd621e100258a24b30cd198f1e0cd68bbc49&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-05-05 08:58:30--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240505T125830Z&X-Amz-Expires=300&X-Amz-Signature=8093acb066386650e6a963d04427bd621e100258a24b30cd198f1e0cd68bbc49&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv.2’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-05 08:58:30 (42.9 MB/s) - ‘taxi_zone_lookup.csv.2’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the zone lookup data\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da09649-e06d-481c-9746-ce1e8a14f9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the CSV as a Spark df\n",
    "zones = spark.read.csv('taxi_zone_lookup.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a2c114-3d6d-4513-a712-5d1da1712411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create temp view for SQL join\n",
    "zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9cf533-9e6d-4ce0-a405-392589a206c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+\n",
      "|      borough|                zone|total_trips|\n",
      "+-------------+--------------------+-----------+\n",
      "|       Queens|         Jamaica Bay|          1|\n",
      "|    Manhattan|Governor's Island...|          2|\n",
      "|     Brooklyn| Green-Wood Cemetery|          5|\n",
      "|       Queens|       Broad Channel|          8|\n",
      "|    Manhattan|     Highbridge Park|         14|\n",
      "|    Manhattan|        Battery Park|         15|\n",
      "|       Queens|Saint Michaels Ce...|         23|\n",
      "|       Queens|Breezy Point/Fort...|         25|\n",
      "|     Brooklyn|Marine Park/Floyd...|         26|\n",
      "|       Queens|        Astoria Park|         29|\n",
      "|    Manhattan|    Inwood Hill Park|         39|\n",
      "|       Queens|       Willets Point|         47|\n",
      "|       Queens|Forest Park/Highl...|         53|\n",
      "|     Brooklyn|  Brooklyn Navy Yard|         57|\n",
      "|        Bronx|        Crotona Park|         62|\n",
      "|        Bronx|        Country Club|         77|\n",
      "|Staten Island|     Freshkills Park|         89|\n",
      "|     Brooklyn|       Prospect Park|         98|\n",
      "|     Brooklyn|     Columbia Street|        105|\n",
      "|     Brooklyn|  South Williamsburg|        110|\n",
      "+-------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL query to show pickups by zone ascending. Top result is the answer to the question.\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "    z.borough,\n",
    "    z.zone,\n",
    "    COUNT(*) AS total_trips\n",
    "    FROM FHV_trips_2019_10 t\n",
    "    INNER JOIN zones z ON t.PUlocationID = z.LocationID\n",
    "    GROUP BY z.zone, z.borough\n",
    "    ORDER BY total_trips\n",
    "    ''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
