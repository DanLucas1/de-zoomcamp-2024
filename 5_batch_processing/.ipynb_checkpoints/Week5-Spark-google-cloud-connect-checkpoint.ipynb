{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7594f972-ad33-40de-8ac4-367a5f2e3820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import requests\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import re\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3affc87-eadf-494a-8c3b-68ecc78395db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# launch a Spark Session with google config\n",
    "\n",
    "# establish home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# specify path to google cloud storage connector jar\n",
    "gcs_connector_jar = os.path.join(home_dir, \"spark\", \"jars\", \"gcs-connector-hadoop3-2.2.22.jar\")\n",
    "\n",
    "# define the path to the service account JSON key file\n",
    "credentials_location = os.path.join(home_dir, '.google', 'service_acct_creds.json')\n",
    "\n",
    "# set up configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", gcs_connector_jar) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c9d57f-942b-4cdf-b2a3-83f8f16a01bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 06:48:57 WARN Utils: Your hostname, thinkpad resolves to a loopback address: 127.0.1.1; using 192.168.1.162 instead (on interface wlp3s0)\n",
      "24/05/08 06:48:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/08 06:48:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create context\n",
    "sc = SparkContext(conf=conf)\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d882907-f9e2-433c-83af-691906b77edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db36939d-668c-493a-9555-98e767ed8021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test writing local CSV to GCS parquet - success\n",
    "# zones_path = os.path.join(home_dir, 'taxi_zone_lookup.csv')\n",
    "# taxi_zone_lookup = spark.read.csv(zones_path, header=True)\n",
    "# taxi_zone_lookup.write.parquet('gs://ny_taxi_storage_413811/zones.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "516789b2-f5bf-4b34-bf0f-0a5ec9163cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# did not work!\n",
    "# test = spark.read.parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-01.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa7a9c75-e233-45c8-9454-0f54fe7272f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# csv_test = requests.get('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdc5d76-52b3-4f4b-9431-08a9fb7fb651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# check = pd.read_parquet('green_tripdata_2020-01.parquet')\n",
    "# pd.concat([check.head(), check.tail()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5459fefa-1181-4a73-8d5a-c7c26b449be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read parquet file from TLC site\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-01.parquet'\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open(\"green_tripdata_2020-01.parquet\", mode=\"wb\") as file:\n",
    "    for chunk in response.iter_content(chunk_size=10 * 1024):\n",
    "        file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7708091-d4f3-4eba-93e2-2433987aaf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "green_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.LongType(), True),\n",
    "    types.StructField('lpep_pickup_datetime', types.TimestampNTZType(), True),\n",
    "    types.StructField('lpep_dropoff_datetime', types.TimestampNTZType(), True),\n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True),\n",
    "    types.StructField('RatecodeID', types.DoubleType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    types.StructField('passenger_count', types.DoubleType(), True),\n",
    "    types.StructField('trip_distance', types.DoubleType(), True),\n",
    "    types.StructField('fare_amount', types.DoubleType(), True),\n",
    "    types.StructField('extra', types.DoubleType(), True),\n",
    "    types.StructField('mta_tax', types.DoubleType(), True),\n",
    "    types.StructField('tip_amount', types.DoubleType(), True),\n",
    "    types.StructField('tolls_amount', types.DoubleType(), True),\n",
    "    types.StructField('ehail_fee', types.IntegerType(), True),\n",
    "    types.StructField('improvement_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('total_amount', types.DoubleType(), True),\n",
    "    types.StructField('payment_type', types.DoubleType(), True),\n",
    "    types.StructField('trip_type', types.DoubleType(), True),\n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e83db27-c3c5-4110-ab9d-1d0cfa30479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to spark df\n",
    "green_trips = spark.read.parquet('green_tripdata_2020-01.parquet', schema = green_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2608accf-7ce1-4660-9331-1e245e1ff67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to clean up column names\n",
    "def column_cleanup(col_name):\n",
    "    col_name = re.sub(r'(?<!_)ID', r'_ID', col_name)\n",
    "    col_name = re.sub(r'(?<!_)PU', r'PU_', col_name)\n",
    "    col_name = re.sub(r'(?<!_)DO', r'DO_', col_name)\n",
    "    col_name = col_name.lower()\n",
    "    return col_name\n",
    "\n",
    "# apply cleanup function to green trips data\n",
    "green_trips = green_trips.select(*[col(c).alias(column_cleanup(c)) for c in green_trips.columns])\n",
    "\n",
    "# remove out-of-range dates\n",
    "green_trips = green_trips.filter((col('lpep_pickup_datetime') >= date(2020, 1, 1)) & (col('lpep_pickup_datetime') < date(2020, 2, 1)))\n",
    "\n",
    "## Alternate method of filtering - use SQL\n",
    "# green_trips.createOrReplaceTempView('trips_df')\n",
    "# spark.sql('''\n",
    "# SELECT *\n",
    "# FROM trips_df\n",
    "# WHERE trip_date BETWEEN '2020-01-01' AND '2022-01-31'\n",
    "# ''')\n",
    "\n",
    "\n",
    "# replace missing values for location IDs with \"unknown\" ID\n",
    "green_trips = green_trips.na.fill({'pu_location_id': 264, 'do_location_id': 264})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41db43d7-53c0-4070-9720-be7d8764247b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+\n",
      "|lpep_pickup_datetime|year|month|day|\n",
      "+--------------------+----+-----+---+\n",
      "| 2020-01-20 00:02:59|2020|    1| 20|\n",
      "| 2020-01-20 00:00:27|2020|    1| 20|\n",
      "| 2020-01-20 00:00:46|2020|    1| 20|\n",
      "| 2020-01-20 00:00:52|2020|    1| 20|\n",
      "| 2020-01-20 00:07:16|2020|    1| 20|\n",
      "| 2020-01-20 00:00:12|2020|    1| 20|\n",
      "| 2020-01-20 00:01:45|2020|    1| 20|\n",
      "| 2020-01-20 00:49:32|2020|    1| 20|\n",
      "| 2020-01-20 00:24:23|2020|    1| 20|\n",
      "| 2020-01-20 00:01:24|2020|    1| 20|\n",
      "+--------------------+----+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add datepart columns for partitioning\n",
    "\n",
    "green_trips = green_trips \\\n",
    "    .withColumn(\"year\", year(col(\"lpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"lpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"lpep_pickup_datetime\")))\n",
    "\n",
    "# green_trips.select([\"lpep_pickup_datetime\",\"year\",\"month\",\"day\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f5dea1f-2fff-48de-99bb-8ec3b1113cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write to cloud storage\n",
    "green_trips.write.format('parquet').partitionBy('year', 'month', 'day').save('gs://ny_taxi_storage_413811/green2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f9e4c-6371-4bdf-aecb-4fe33d9e98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that Spark can also read the parquet files that have been written to GCS\n",
    "read_test = spark.read.parquet('gs://ny_taxi_storage_413811/green2/*/*')\n",
    "read_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5ef8566-9f20-47e9-b9c0-93ab5af6ff57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d275075d-17dd-43da-a572-12b0a7651715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "todo = '''\n",
    "\n",
    "RECREATE BACKFILL (green only ok)\n",
    "\n",
    "1) Data fetch:\n",
    "[x] download files (done requests.get)\n",
    "[ ] clean data (translate pandas -> Spark)\n",
    "    [x] filter for out-of-range dates\n",
    "    [x] column name cleanup\n",
    "    [x] create date cast column for repartitioning\n",
    "    [x] fix missing/invalid values\n",
    "[x] repartition\n",
    "[x] write to GCS parquet\n",
    "\n",
    "2) Transfer to BQ:\n",
    "[ ] read from GCS (basically done)\n",
    "[ ] clean data (mostly schemas)\n",
    "[ ] Connect to BQ\n",
    "[ ] write datasets to tables\n",
    "    [ ] partitioning/clustering\n",
    "    [ ] schema\n",
    "    \n",
    "3) Batch\n",
    "[ ] Parametrize (month / year / taxi-type)\n",
    "[ ] Run with dataproc\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
